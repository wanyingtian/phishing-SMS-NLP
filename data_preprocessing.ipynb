{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize English text\n",
    "def clean_english_text(text):\n",
    "    text = re.sub(r'http\\S+', 'URL', text)  # Replace URLs with URL\n",
    "    text = re.sub(r'\\S+@\\S+', 'EMAIL', text)  # Replace email addresses with EMAIL\n",
    "    text = re.sub(r'\\d{3,}', 'PHONENUMBER', text)      # Replace phone numbers with PHONE\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Function to check for phishing indicators and clean text\n",
    "def check_and_clean_text(text):\n",
    "    has_url = bool(re.search(r'URL', text))\n",
    "    has_email = bool(re.search(r'EMAIL', text))\n",
    "    has_phone = bool(re.search(r'PHONE', text))\n",
    "    cleaned_text = clean_english_text(text)\n",
    "    return cleaned_text, has_url, has_email, has_phone\n",
    "\n",
    "# Load Mendeley dataset\n",
    "def load_mendeley_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['LABEL'] = df['LABEL'].map({'ham': 0, 'spam': 1, 'Smishing': 2})\n",
    "    df['CLEANED_TEXT'], df['HAS_URL'], df['HAS_EMAIL'], df['HAS_PHONE'] = zip(*df['TEXT'].map(check_and_clean_text))\n",
    "    df['TOKENIZED_TEXT'] = df['CLEANED_TEXT'].apply(lambda x: x.split())\n",
    "    return df\n",
    "\n",
    "# Load SPAM SMS dataset\n",
    "def load_spam_sms(file_path):\n",
    "    with open(file_path, 'r', encoding='latin-1') as f:\n",
    "        data = [line.strip().split('\\t') for line in f.readlines()]\n",
    "    df = pd.DataFrame(data, columns=['LABEL', 'TEXT'])\n",
    "    df['LABEL'] = df['LABEL'].map({'ham': 0, 'spam': 1, 'Smishing': 2})\n",
    "    df['CLEANED_TEXT'], df['HAS_URL'], df['HAS_EMAIL'], df['HAS_PHONE'] = zip(*df['TEXT'].map(check_and_clean_text))\n",
    "    df['TOKENIZED_TEXT'] = df['CLEANED_TEXT'].apply(lambda x: x.split())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>URL</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>PHONE</th>\n",
       "      <th>CLEANED_TEXT</th>\n",
       "      <th>HAS_URL</th>\n",
       "      <th>HAS_EMAIL</th>\n",
       "      <th>HAS_PHONE</th>\n",
       "      <th>TOKENIZED_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your opinion about me? 1. Over 2. Jada 3. Kusr...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>your opinion about me 1 over 2 jada 3 kusruthi...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[your, opinion, about, me, 1, over, 2, jada, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>What's up? Do you want me to come online? If y...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>whats up do you want me to come online if you ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[whats, up, do, you, want, me, to, come, onlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>So u workin overtime nigpun?</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>so u workin overtime nigpun</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[so, u, workin, overtime, nigpun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Also sir, i sent you an email about how to log...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>also sir i sent you an email about how to log ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[also, sir, i, sent, you, an, email, about, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Please Stay At Home. To encourage the notion o...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>please stay at home to encourage the notion of...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[please, stay, at, home, to, encourage, the, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[this, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Will Ã¼ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>will ã¼ b going to esplanade fr home</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[will, ã¼, b, going, to, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pity  was in mood for that soany other suggest...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[pity, was, in, mood, for, that, soany, other,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[the, guy, did, some, bitching, but, i, acted,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[rofl, its, true, to, its, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11545 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL                                               TEXT  URL EMAIL  \\\n",
       "0       0.0  Your opinion about me? 1. Over 2. Jada 3. Kusr...   No    No   \n",
       "1       0.0  What's up? Do you want me to come online? If y...   No    No   \n",
       "2       0.0                       So u workin overtime nigpun?   No    No   \n",
       "3       0.0  Also sir, i sent you an email about how to log...   No    No   \n",
       "4       2.0  Please Stay At Home. To encourage the notion o...   No    No   \n",
       "...     ...                                                ...  ...   ...   \n",
       "5569    1.0  This is the 2nd time we have tried 2 contact u...  NaN   NaN   \n",
       "5570    0.0              Will Ã¼ b going to esplanade fr home?  NaN   NaN   \n",
       "5571    0.0  Pity, * was in mood for that. So...any other s...  NaN   NaN   \n",
       "5572    0.0  The guy did some bitching but I acted like i'd...  NaN   NaN   \n",
       "5573    0.0                         Rofl. Its true to its name  NaN   NaN   \n",
       "\n",
       "     PHONE                                       CLEANED_TEXT  HAS_URL  \\\n",
       "0       No  your opinion about me 1 over 2 jada 3 kusruthi...    False   \n",
       "1       No  whats up do you want me to come online if you ...    False   \n",
       "2       No                        so u workin overtime nigpun    False   \n",
       "3       No  also sir i sent you an email about how to log ...    False   \n",
       "4       No  please stay at home to encourage the notion of...    False   \n",
       "...    ...                                                ...      ...   \n",
       "5569   NaN  this is the 2nd time we have tried 2 contact u...    False   \n",
       "5570   NaN               will ã¼ b going to esplanade fr home    False   \n",
       "5571   NaN  pity  was in mood for that soany other suggest...    False   \n",
       "5572   NaN  the guy did some bitching but i acted like id ...    False   \n",
       "5573   NaN                          rofl its true to its name    False   \n",
       "\n",
       "      HAS_EMAIL  HAS_PHONE                                     TOKENIZED_TEXT  \n",
       "0         False      False  [your, opinion, about, me, 1, over, 2, jada, 3...  \n",
       "1         False      False  [whats, up, do, you, want, me, to, come, onlin...  \n",
       "2         False      False                  [so, u, workin, overtime, nigpun]  \n",
       "3         False      False  [also, sir, i, sent, you, an, email, about, ho...  \n",
       "4         False      False  [please, stay, at, home, to, encourage, the, n...  \n",
       "...         ...        ...                                                ...  \n",
       "5569      False      False  [this, is, the, 2nd, time, we, have, tried, 2,...  \n",
       "5570      False      False      [will, ã¼, b, going, to, esplanade, fr, home]  \n",
       "5571      False      False  [pity, was, in, mood, for, that, soany, other,...  \n",
       "5572      False      False  [the, guy, did, some, bitching, but, i, acted,...  \n",
       "5573      False      False                   [rofl, its, true, to, its, name]  \n",
       "\n",
       "[11545 rows x 10 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mendeley_df = load_mendeley_csv('mendeley/Dataset_5971.csv')\n",
    "uci_df = load_spam_sms('uci/SMSSpamCollection')\n",
    "english_df = pd.concat([mendeley_df,uci_df])\n",
    "english_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "stopwords_path = 'ChineseTextClassification/data/hit_stopwords.txt'\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Function to clean and tokenize Chinese text\n",
    "def clean_chinese_text(text):\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'x', '', text)  # Remove special characters x\n",
    "    return text\n",
    "# Function to check for phishing indicators and clean text\n",
    "def check_and_clean_chinese_text(text):\n",
    "    has_url = bool(re.search(r'URL', text))\n",
    "    has_email = bool(re.search(r'EMAIL', text))\n",
    "    has_phone = bool(re.search(r'PHONE', text))\n",
    "    cleaned_text = clean_chinese_text(text)\n",
    "    return cleaned_text, has_url, has_email, has_phone\n",
    "# Load Chinese Text Classification dataset\n",
    "def load_chinese_text_classification(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['LABEL', 'TEXT'])\n",
    "    df['CLEANED_TEXT'], df['HAS_URL'], df['HAS_EMAIL'], df['HAS_PHONE'] = zip(*df['TEXT'].map(check_and_clean_chinese_text))\n",
    "    df['TOKENIZED_TEXT'] = df['CLEANED_TEXT'].apply(lambda x: [word for word in jieba.cut(x) if word not in stopwords])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load FBS SMS Dataset with labels based on filenames\n",
    "def load_fbs_sms(directory_path):\n",
    "    data = []\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        label = file_name.split('.')[0]  # Assuming the label is the part of the filename before the extension\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    data.append([label, line])  # Use the label from the filename\n",
    "    df = pd.DataFrame(data, columns=['LABEL', 'TEXT'])\n",
    "    df['CLEANED_TEXT'], df['HAS_URL'], df['HAS_EMAIL'], df['HAS_PHONE'] = zip(*df['TEXT'].map(check_and_clean_chinese_text))\n",
    "    df['TOKENIZED_TEXT'] = df['CLEANED_TEXT'].apply(lambda x: [word for word in jieba.cut(x) if word not in stopwords])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_text_classification_df = load_chinese_text_classification('ChineseTextClassification/data/train.txt')\n",
    "fbs_sms_df = load_fbs_sms('FBS_SMS_Dataset/data')\n",
    "# Update labels for FBS SMS dataset\n",
    "fbs_sms_df['LABEL'] = fbs_sms_df['LABEL'].apply(lambda x: 1 if x.startswith('AD') else (2 if x.startswith('FR') else x))\n",
    "chinese_df = pd.concat([chinese_text_classification_df,fbs_sms_df])\n",
    "chinese_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CLEANED_TEXT</th>\n",
       "      <th>HAS_URL</th>\n",
       "      <th>HAS_EMAIL</th>\n",
       "      <th>HAS_PHONE</th>\n",
       "      <th>TOKENIZED_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一</td>\n",
       "      <td>商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[商业秘密, 秘密性, 维系, 商业价值, 垄断, 地位, 前提条件]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>南口阿玛施新春第一批限量春装到店啦         春暖花开淑女裙、冰蓝色公主衫  ...</td>\n",
       "      <td>南口阿玛施新春第一批限量春装到店啦         春暖花开淑女裙冰蓝色公主衫   气质粉小...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[南口, 阿玛施, 新春, 第一批, 限量, 春装, 店,  ,  ,  ,  ,  ,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>带给我们大常州一场壮观的视觉盛宴</td>\n",
       "      <td>带给我们大常州一场壮观的视觉盛宴</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[带给, 大, 常州, 一场, 壮观, 视觉, 盛宴]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>有原因不明的泌尿系统结石等</td>\n",
       "      <td>有原因不明的泌尿系统结石等</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[原因, 不明, 泌尿系统, 结石]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>23年从盐城拉回来的麻麻的嫁妆</td>\n",
       "      <td>23年从盐城拉回来的麻麻的嫁妆</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[23, 年, 盐城, 拉回来, 麻麻, 嫁妆]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11391</th>\n",
       "      <td>1</td>\n",
       "      <td>PLACE 移动 两周年 感恩 大 回馈 套餐 订购 有 礼 活动 惊喜 来 袭 本月 回复...</td>\n",
       "      <td>place 移动 两周年 感恩 大 回馈 套餐 订购 有 礼 活动 惊喜 来 袭 本月 回复...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[place,  , 移动,  , 两周年,  , 感恩,  , 大,  , 回馈,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11392</th>\n",
       "      <td>1</td>\n",
       "      <td>您好 请 回复 您 的 或 位 的 身份证 号码 信息 超级 流量 NAME 给 你 一次 ...</td>\n",
       "      <td>您好 请 回复 您 的 或 位 的 身份证 号码 信息 超级 流量 name 给 你 一次 ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[您好,  , 请,  , 回复,  ,  ,  ,  , 位,  ,  , 身份证,  ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11393</th>\n",
       "      <td>1</td>\n",
       "      <td>辞旧迎新 NAME 的 流量 嗨 翻天 百万 流量 红包 疯狂 抢 疯狂 派 开心 答题 N...</td>\n",
       "      <td>辞旧迎新 name 的 流量 嗨 翻天 百万 流量 红包 疯狂 抢 疯狂 派 开心 答题 n...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[辞旧迎新,  , name,  ,  , 流量,  , 嗨,  , 翻天,  , 百万, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11394</th>\n",
       "      <td>1</td>\n",
       "      <td>游戏 NAME 尊贵 的 用户 您好 PLACE 联通 为 您 提供 新花 千骨 独家 礼包...</td>\n",
       "      <td>游戏 name 尊贵 的 用户 您好 place 联通 为 您 提供 新花 千骨 独家 礼包...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[游戏,  , name,  , 尊贵,  ,  , 用户,  , 您好,  , place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11395</th>\n",
       "      <td>1</td>\n",
       "      <td>PLACE 通讯 年度 充值 优惠 PLACE 网络 全面 升级 邀请 您 充 DIGIT ...</td>\n",
       "      <td>place 通讯 年度 充值 优惠 place 网络 全面 升级 邀请 您 充 digit ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[place,  , 通讯,  , 年度,  , 充值,  , 优惠,  , place, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>578755 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LABEL                                               TEXT  \\\n",
       "0          0                      商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一   \n",
       "1          1  南口阿玛施新春第一批限量春装到店啦         春暖花开淑女裙、冰蓝色公主衫  ...   \n",
       "2          0                                   带给我们大常州一场壮观的视觉盛宴   \n",
       "3          0                                      有原因不明的泌尿系统结石等   \n",
       "4          0                                    23年从盐城拉回来的麻麻的嫁妆   \n",
       "...      ...                                                ...   \n",
       "11391      1  PLACE 移动 两周年 感恩 大 回馈 套餐 订购 有 礼 活动 惊喜 来 袭 本月 回复...   \n",
       "11392      1  您好 请 回复 您 的 或 位 的 身份证 号码 信息 超级 流量 NAME 给 你 一次 ...   \n",
       "11393      1  辞旧迎新 NAME 的 流量 嗨 翻天 百万 流量 红包 疯狂 抢 疯狂 派 开心 答题 N...   \n",
       "11394      1  游戏 NAME 尊贵 的 用户 您好 PLACE 联通 为 您 提供 新花 千骨 独家 礼包...   \n",
       "11395      1  PLACE 通讯 年度 充值 优惠 PLACE 网络 全面 升级 邀请 您 充 DIGIT ...   \n",
       "\n",
       "                                            CLEANED_TEXT  HAS_URL  HAS_EMAIL  \\\n",
       "0                          商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一    False      False   \n",
       "1      南口阿玛施新春第一批限量春装到店啦         春暖花开淑女裙冰蓝色公主衫   气质粉小...    False      False   \n",
       "2                                       带给我们大常州一场壮观的视觉盛宴    False      False   \n",
       "3                                          有原因不明的泌尿系统结石等    False      False   \n",
       "4                                        23年从盐城拉回来的麻麻的嫁妆    False      False   \n",
       "...                                                  ...      ...        ...   \n",
       "11391  place 移动 两周年 感恩 大 回馈 套餐 订购 有 礼 活动 惊喜 来 袭 本月 回复...    False      False   \n",
       "11392  您好 请 回复 您 的 或 位 的 身份证 号码 信息 超级 流量 name 给 你 一次 ...     True      False   \n",
       "11393  辞旧迎新 name 的 流量 嗨 翻天 百万 流量 红包 疯狂 抢 疯狂 派 开心 答题 n...     True      False   \n",
       "11394  游戏 name 尊贵 的 用户 您好 place 联通 为 您 提供 新花 千骨 独家 礼包...     True      False   \n",
       "11395  place 通讯 年度 充值 优惠 place 网络 全面 升级 邀请 您 充 digit ...    False      False   \n",
       "\n",
       "       HAS_PHONE                                     TOKENIZED_TEXT  \n",
       "0          False                [商业秘密, 秘密性, 维系, 商业价值, 垄断, 地位, 前提条件]  \n",
       "1          False  [南口, 阿玛施, 新春, 第一批, 限量, 春装, 店,  ,  ,  ,  ,  ,  ...  \n",
       "2          False                        [带给, 大, 常州, 一场, 壮观, 视觉, 盛宴]  \n",
       "3          False                                 [原因, 不明, 泌尿系统, 结石]  \n",
       "4          False                           [23, 年, 盐城, 拉回来, 麻麻, 嫁妆]  \n",
       "...          ...                                                ...  \n",
       "11391      False  [place,  , 移动,  , 两周年,  , 感恩,  , 大,  , 回馈,  , ...  \n",
       "11392      False  [您好,  , 请,  , 回复,  ,  ,  ,  , 位,  ,  , 身份证,  ,...  \n",
       "11393      False  [辞旧迎新,  , name,  ,  , 流量,  , 嗨,  , 翻天,  , 百万, ...  \n",
       "11394      False  [游戏,  , name,  , 尊贵,  ,  , 用户,  , 您好,  , place...  \n",
       "11395      False  [place,  , 通讯,  , 年度,  , 充值,  , 优惠,  , place, ...  \n",
       "\n",
       "[578755 rows x 7 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames have been saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Save each DataFrame to a CSV file\n",
    "mendeley_df.to_csv('processed_data/mendeley_df.csv', index=False)\n",
    "uci_df.to_csv('processed_data/uci_df.csv', index=False)\n",
    "chinese_text_classification_df.to_csv('processed_data/chinese_text_classification_df.csv', index=False)\n",
    "fbs_sms_df.to_csv('processed_data/fbs_sms_df.csv', index=False)\n",
    "\n",
    "print(\"All DataFrames have been saved as CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Words in Mendeley Dataset (Label 1)\n",
      "Word                 Frequency\n",
      "==============================\n",
      "phonenumber          513\n",
      "to                   445\n",
      "a                    160\n",
      "the                  157\n",
      "free                 151\n",
      "for                  149\n",
      "your                 137\n",
      "now                  126\n",
      "txt                  119\n",
      "call                 115\n",
      "2                    109\n",
      "and                  103\n",
      "you                  101\n",
      "on                   94\n",
      "ur                   93\n",
      "or                   91\n",
      "stop                 90\n",
      "text                 81\n",
      "get                  80\n",
      "4                    79\n",
      "\n",
      "Top 20 Words in Mendeley Dataset (Label 2)\n",
      "Word                 Frequency\n",
      "==============================\n",
      "phonenumber          997\n",
      "to                   522\n",
      "call                 386\n",
      "you                  363\n",
      "a                    355\n",
      "your                 322\n",
      "have                 173\n",
      "claim                166\n",
      "for                  153\n",
      "prize                138\n",
      "is                   134\n",
      "or                   132\n",
      "customer             130\n",
      "now                  118\n",
      "free                 118\n",
      "the                  117\n",
      "on                   113\n",
      "are                  107\n",
      "please               104\n",
      "mobile               104\n",
      "\n",
      "Top 20 Words in UCI SMS Dataset (Label 1)\n",
      "Word                 Frequency\n",
      "==============================\n",
      "phonenumber          939\n",
      "to                   689\n",
      "a                    378\n",
      "call                 347\n",
      "you                  287\n",
      "your                 263\n",
      "free                 216\n",
      "the                  204\n",
      "for                  203\n",
      "âphonenumber         194\n",
      "now                  189\n",
      "or                   188\n",
      "2                    173\n",
      "is                   158\n",
      "txt                  150\n",
      "u                    147\n",
      "on                   145\n",
      "ur                   144\n",
      "have                 135\n",
      "from                 128\n",
      "\n",
      "Top 20 Words in Chinese Text Classification Dataset (Label 1)\n",
      "Word                 Frequency\n",
      "==============================\n",
      "0                    8419\n",
      "1                    943\n",
      "电话                   342\n",
      "您好                   216\n",
      "联系电话                 142\n",
      "的                    131\n",
      "你好                   122\n",
      "农行                   81\n",
      "元                    79\n",
      "je                   78\n",
      "qq                   76\n",
      "地址                   74\n",
      "m                    74\n",
      "送                    65\n",
      "平安易贷可帮您获取所需资金无须抵押手续简便快速快来申请吧在申请时输入我的邀请码邀请码有效期至年月日 61\n",
      "好消息                  54\n",
      "上                    54\n",
      "好                    54\n",
      "是                    53\n",
      "学海教育个性化教育第一品牌一线教师根据学生学习情况制订适合的教学方案查漏补缺电话 52\n",
      "\n",
      "Top 20 Words in FBS SMS Dataset (Label 1)\n",
      "Word                 Frequency\n",
      "==============================\n",
      "digit                14394\n",
      "name                 8033\n",
      "place                6521\n",
      "元                    4947\n",
      "的                    4315\n",
      "退订                   3331\n",
      "您                    3158\n",
      "url                  2715\n",
      "流量                   2691\n",
      "回                    2190\n",
      "送                    2095\n",
      "月                    2090\n",
      "td                   2027\n",
      "日                    1994\n",
      "回复                   1571\n",
      "会员                   1550\n",
      "手机                   1347\n",
      "等                    1331\n",
      "折                    1295\n",
      "可                    1290\n",
      "\n",
      "Top 20 Words in FBS SMS Dataset (Label 2)\n",
      "Word                 Frequency\n",
      "==============================\n",
      "的                    4691\n",
      "您                    3127\n",
      "url                  2342\n",
      "请                    2150\n",
      "尊敬                   2148\n",
      "digit                1710\n",
      "用户                   1708\n",
      "name                 1449\n",
      "已                    1147\n",
      "积分                   1084\n",
      "失效                   1072\n",
      "兑换                   966\n",
      "我行                   962\n",
      "现金                   914\n",
      "逾期                   824\n",
      "领取                   819\n",
      "工商银行                 810\n",
      "可兑换                  724\n",
      "place                669\n",
      "登陆                   662\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to get top N words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Function to print top words\n",
    "def print_top_words(words_freq, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"{'Word':<20} {'Frequency'}\")\n",
    "    print(\"=\"*30)\n",
    "    for word, freq in words_freq:\n",
    "        print(f\"{word:<20} {freq}\")\n",
    "\n",
    "# Analyze and print top words for each label in the dataframe\n",
    "def analyze_dataframe_by_label(df, title, labels):\n",
    "    for label in labels:\n",
    "        filtered_df = df[df['LABEL'] == label]\n",
    "        words_freq = get_top_n_words(filtered_df['CLEANED_TEXT'], 20)\n",
    "        print_top_words(words_freq, f\"{title} (Label {label})\")\n",
    "\n",
    "\n",
    "\n",
    "# Analyze Mendeley dataset\n",
    "analyze_dataframe_by_label(mendeley_df, 'Top 20 Words in Mendeley Dataset', [1, 2])\n",
    "\n",
    "# Analyze UCI SMS dataset\n",
    "analyze_dataframe_by_label(uci_df, 'Top 20 Words in UCI SMS Dataset', [1])\n",
    "\n",
    "# Analyze Chinese Text Classification dataset\n",
    "analyze_dataframe_by_label(chinese_text_classification_df, 'Top 20 Words in Chinese Text Classification Dataset', [1])\n",
    "\n",
    "# Analyze FBS SMS dataset\n",
    "analyze_dataframe_by_label(fbs_sms_df, 'Top 20 Words in FBS SMS Dataset', [1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in Mendeley Dataset\n",
      "Word                 Frequency\n",
      "==============================\n",
      "example              1\n",
      "text                 1\n",
      "for                  1\n",
      "mendeley             1\n",
      "dataset              1\n",
      "Top 20 Words in SPAM SMS Dataset\n",
      "Word                 Frequency\n",
      "==============================\n",
      "phone                2350\n",
      "to                   2251\n",
      "i                    2239\n",
      "you                  2128\n",
      "a                    1442\n",
      "the                  1333\n",
      "u                    1132\n",
      "and                  971\n",
      "is                   893\n",
      "in                   888\n",
      "me                   791\n",
      "my                   757\n",
      "for                  710\n",
      "your                 677\n",
      "it                   622\n",
      "of                   620\n",
      "call                 578\n",
      "have                 576\n",
      "on                   536\n",
      "that                 514\n",
      "Top 20 Words in Chinese Text Classification Dataset\n",
      "Word                 Frequency\n",
      "==============================\n",
      "0                    29376\n",
      "1                    3269\n",
      "xxxx                 1451\n",
      "x                    782\n",
      "xxxxxxxxxxx          665\n",
      "xxx                  401\n",
      "xx                   306\n",
      "您好                   226\n",
      "xxxxxxx              216\n",
      "电话xxxxxxxxxxx        175\n",
      "xxxxxx               165\n",
      "xxxxx                152\n",
      "的                    135\n",
      "xxxxxxxxxxxxxxxxxxx  134\n",
      "你好                   122\n",
      "xxxxxxxx             118\n",
      "联系电话xxxxxxxxxxx      113\n",
      "je                   80\n",
      "地址                   74\n",
      "平安易贷可帮您获取所需资金无须抵押手续简便快速快来申请吧在申请时输入我的邀请码xxxxxx邀请码有效期至xxxx年xx月xx日 64\n",
      "Top 20 Words in FBS SMS Dataset\n",
      "Word                 Frequency\n",
      "==============================\n",
      "digit                16104\n",
      "name                 9482\n",
      "的                    9006\n",
      "place                7190\n",
      "您                    6285\n",
      "元                    5524\n",
      "url                  5057\n",
      "退订                   3473\n",
      "尊敬                   3078\n",
      "流量                   2984\n",
      "请                    2775\n",
      "回                    2304\n",
      "月                    2216\n",
      "用户                   2183\n",
      "日                    2117\n",
      "送                    2106\n",
      "td                   2043\n",
      "手机                   1989\n",
      "已                    1750\n",
      "积分                   1660\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to get top N words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Function to print top words\n",
    "def print_top_words(words_freq, title):\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'Word':<20} {'Frequency'}\")\n",
    "    print(\"=\"*30)\n",
    "    for word, freq in words_freq:\n",
    "        print(f\"{word:<20} {freq}\")\n",
    "\n",
    "# Analyze and print top words for each dataframe\n",
    "def analyze_dataframe(df, title):\n",
    "    words_freq = get_top_n_words(df['CLEANED_TEXT'], 20)\n",
    "    print_top_words(words_freq, title)\n",
    "\n",
    "\n",
    "\n",
    "# Analyze Mendeley dataset\n",
    "analyze_dataframe(mendeley_df, 'Top 20 Words in Mendeley Dataset')\n",
    "\n",
    "# Analyze SPAM SMS dataset\n",
    "analyze_dataframe(uci_df, 'Top 20 Words in SPAM SMS Dataset')\n",
    "\n",
    "# Analyze Chinese Text Classification dataset\n",
    "analyze_dataframe(chinese_text_classification_df, 'Top 20 Words in Chinese Text Classification Dataset')\n",
    "\n",
    "# Analyze FBS SMS dataset\n",
    "analyze_dataframe(fbs_sms_df, 'Top 20 Words in FBS SMS Dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
